{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7db57f5-bbf6-4b02-ad7d-66b105e0d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel,ViTModel,VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2559a0f4-4de5-40d6-8c3d-15268760bda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c764b01653e4edbb6e0b9d255d1318c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vizwiz = load_dataset(\"Multimodal-Fatima/VizWiz_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79e0df-3a3c-4120-a76c-17f18ae73278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'image', 'filename', 'question', 'answers', 'answers_original', 'answer_type', 'answerable', 'id_image', 'clip_tags_ViT_L_14', 'clip_tags_LAION_ViT_H_14_2B', 'blip_caption_beam_5', 'LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14', 'LLM_Description_gpt3_downstream_tasks_visual_genome_LAION-ViT-H-14-2B', 'DETA_detections_deta_swin_large_o365_coco_classes'],\n",
      "        num_rows: 14776\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'image', 'filename', 'question', 'answers', 'answers_original', 'answer_type', 'answerable', 'id_image', 'clip_tags_ViT_L_14', 'clip_tags_LAION_ViT_H_14_2B', 'blip_caption_beam_5', 'LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14', 'LLM_Description_gpt3_downstream_tasks_visual_genome_LAION-ViT-H-14-2B', 'DETA_detections_deta_swin_large_o365_coco_classes'],\n",
      "        num_rows: 1642\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'image', 'filename', 'question', 'answers', 'answers_original', 'answer_type', 'answerable', 'id_image', 'clip_tags_ViT_L_14', 'clip_tags_LAION_ViT_H_14_2B', 'blip_caption_beam_5', 'LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14', 'LLM_Description_gpt3_downstream_tasks_visual_genome_LAION-ViT-H-14-2B', 'DETA_detections_deta_swin_large_o365_coco_classes'],\n",
      "        num_rows: 4105\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_test_split = vizwiz['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "train_valid_split = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_valid_split[\"train\"]\n",
    "valid_dataset = train_valid_split[\"test\"]\n",
    "\n",
    "split_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": valid_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(split_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c463bfb5-2ff4-4dc2-af44-8c2aee15e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "684aa9e0-0d75-45e0-bcb3-9d26c435980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = split_dataset['train'].remove_columns([\n",
    "    'id', 'filename', 'answers_original', 'answer_type', 'answerable','id_image', 'clip_tags_ViT_L_14', 'clip_tags_LAION_ViT_H_14_2B',\n",
    "    'blip_caption_beam_5', \n",
    "    'LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14', \n",
    "    'LLM_Description_gpt3_downstream_tasks_visual_genome_LAION-ViT-H-14-2B', \n",
    "    'DETA_detections_deta_swin_large_o365_coco_classes'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a625c634-bd88-4ea6-87fb-6b698c2091b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (q_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b3d1d1f-8447-402a-a80e-4dfdd0553ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d92ce432f74507bda4421d89a73c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    }
   ],
   "source": [
    "def image_captions(data):\n",
    "\n",
    "    ''' Preprocesses the data by adding captions to the images.\n",
    "\n",
    "        Arguments : \n",
    "        -data : a dict \n",
    "\n",
    "        Outputs : \n",
    "            An upgraded dict with the added captions for each image.\n",
    "    '''\n",
    "    images = []\n",
    "    images.append(data['image'].convert(\"RGB\"))\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    gen_kwargs = {\"max_length\": 16, \"num_beams\": 4}\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    captions = [pred.strip() for pred in preds]\n",
    "    images = transform(data['image'].convert(\"RGB\"))\n",
    "    question = data['question']\n",
    "    answers = data['answers'][0] \n",
    "    \n",
    "\n",
    "    return {'image' : images,\n",
    "           'question': question,\n",
    "            'answers': answers,\n",
    "            'captions': captions\n",
    "           }\n",
    "\n",
    "dataset_dict = train_dataset.map(preprocess_images)\n",
    "pytorch_train = dataset_dict.with_format(type='torch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92c9afb8-329d-4c05-8018-1a81a92ed180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[0.2353, 0.2157, 0.2078,  ..., 0.2941, 0.2824, 0.2824],\n",
       "          [0.2471, 0.2392, 0.2235,  ..., 0.2902, 0.2863, 0.2902],\n",
       "          [0.2588, 0.2588, 0.2706,  ..., 0.2902, 0.2941, 0.2902],\n",
       "          ...,\n",
       "          [0.1333, 0.1294, 0.1294,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.1294, 0.1216, 0.1176,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.1294, 0.1176, 0.1137,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.1725, 0.1216, 0.1098,  ..., 0.1843, 0.1725, 0.1725],\n",
       "          [0.2314, 0.1882, 0.1333,  ..., 0.1804, 0.1765, 0.1804],\n",
       "          [0.2392, 0.2392, 0.2196,  ..., 0.1804, 0.1804, 0.1804],\n",
       "          ...,\n",
       "          [0.0667, 0.0627, 0.0627,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0627, 0.0549, 0.0510,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0627, 0.0510, 0.0471,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.1490, 0.1020, 0.0863,  ..., 0.1294, 0.1176, 0.1176],\n",
       "          [0.2039, 0.1647, 0.1137,  ..., 0.1255, 0.1216, 0.1255],\n",
       "          [0.2196, 0.2157, 0.2000,  ..., 0.1216, 0.1255, 0.1216],\n",
       "          ...,\n",
       "          [0.0392, 0.0353, 0.0353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0353, 0.0275, 0.0235,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0353, 0.0235, 0.0196,  ..., 1.0000, 1.0000, 1.0000]]]),\n",
       " 'question': 'Can you tell me who this piece of mail is from? Thank you.',\n",
       " 'answers': 'unanswerable',\n",
       " 'captions': ['a person holding a piece of paper on top of a box']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_train[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd52bc2-7318-4daf-8a8e-6959048fac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = split_dataset['validation'].remove_columns([\n",
    "    'id', 'filename', 'answers_original', 'answer_type', 'answerable','id_image', 'clip_tags_ViT_L_14', 'clip_tags_LAION_ViT_H_14_2B',\n",
    "    'blip_caption_beam_5', \n",
    "    'LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14', \n",
    "    'LLM_Description_gpt3_downstream_tasks_visual_genome_LAION-ViT-H-14-2B', \n",
    "    'DETA_detections_deta_swin_large_o365_coco_classes'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a71f94c-58f0-47b3-bf18-0eae9155d43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2757a3421b4a7d9f5207a2b21d654e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1642 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict = valid.map(preprocess_images)\n",
    "pytorch_valid = dataset_dict.with_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb83d743-847c-45ad-8683-269b8cee00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        \n",
    "        # Image Encoder \n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        self.vision_fc = nn.Linear(768, 512)  \n",
    "        \n",
    "        # Encoder for Question\n",
    "        self.question_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.question_fc = nn.Linear(768, 512)\n",
    "\n",
    "        # Encoder for Caption \n",
    "        self.caption_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.caption_fc = nn.Linear(768, 512)\n",
    "\n",
    "        self.projection_fc = nn.Linear(1536, 768)\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "        # Answer Generator \n",
    "        self.answer_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "    def forward(self, image, questions, captions,answers):\n",
    "        # Encode the image\n",
    "        image_features = self.vision_encoder(image).pooler_output\n",
    "        image_features = self.vision_fc(image_features)\n",
    "\n",
    "        # Encode the question\n",
    "        question_outputs = self.question_encoder(input_ids=questions['input_ids'], attention_mask=questions['attention_mask'])\n",
    "        question_features = self.question_fc(question_outputs.pooler_output)#.to(device))\n",
    "\n",
    "        # Encode the caption\n",
    "        caption_outputs = self.caption_encoder(input_ids=captions['input_ids'], attention_mask=captions['attention_mask'])\n",
    "        caption_features = self.caption_fc(caption_outputs.pooler_output)#.to(device))\n",
    "        \n",
    "\n",
    "        #print(f\"Image Features Shape: {image_features.shape}\")\n",
    "        #print(f\"Question Features Shape: {question_features.shape}\")\n",
    "        #print(f\"Caption Features Shape: {caption_features.shape}\")\n",
    "\n",
    "        # Concatenate and fuse features\n",
    "        combined_features = torch.cat((image_features, question_features, caption_features), dim=1)\n",
    "        combined_features = self.projection_fc(combined_features)#.to(device))\n",
    "        \n",
    "        answer_input_ids = self.tokenizer(answers, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        #print(answer_input_ids)\n",
    "\n",
    "        # Reshape combined features for GPT-2 input\n",
    "        sequence_length = answer_input_ids.size(1)  \n",
    "        combined_features = combined_features.unsqueeze(1).repeat(1, sequence_length, 1)  # Shape: (batch_size, sequence_length, 768)\n",
    "        #print(combined_features)\n",
    "        \n",
    "        \n",
    "        # Generate output using the combined context with actual answers as labels\n",
    "        outputs = self.answer_decoder(inputs_embeds=combined_features, labels=answer_input_ids)  # Use actual answers for training\n",
    "        \n",
    "\n",
    "        return outputs.loss, outputs.logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e40c3b-9310-4674-976e-5c1ca30a0292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93342efc-3080-4273-81c0-bd63c1756d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "question_caption_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "answer_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "answer_tokenizer.pad_token = answer_tokenizer.eos_token \n",
    "train_loader = DataLoader(pytorch_valid, batch_size=16, shuffle=True)\n",
    "modelvqa = MultimodalVQAModel()\n",
    "device = torch.device(\"cuda:0\")\n",
    "modelvqa.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "checkpoint_path=\"model_checkpointvizwiz.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e4f980c-92f5-4b96-a8ac-651ba875c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[0.8471, 0.8431, 0.8275,  ..., 0.7098, 0.7020, 0.7059],\n",
      "         [0.8353, 0.8275, 0.8157,  ..., 0.7137, 0.7098, 0.7098],\n",
      "         [0.8157, 0.8078, 0.8039,  ..., 0.7137, 0.7098, 0.7098],\n",
      "         ...,\n",
      "         [0.1529, 0.1451, 0.1569,  ..., 0.8118, 0.8039, 0.7882],\n",
      "         [0.1529, 0.1686, 0.1843,  ..., 0.8275, 0.8235, 0.8196],\n",
      "         [0.2039, 0.2039, 0.1882,  ..., 0.8275, 0.8275, 0.8275]],\n",
      "\n",
      "        [[0.3255, 0.3176, 0.3137,  ..., 0.6000, 0.6000, 0.6000],\n",
      "         [0.3137, 0.3059, 0.3020,  ..., 0.6078, 0.6039, 0.5961],\n",
      "         [0.3059, 0.2980, 0.2980,  ..., 0.6078, 0.6078, 0.6000],\n",
      "         ...,\n",
      "         [0.1569, 0.1412, 0.1490,  ..., 0.7255, 0.7176, 0.7059],\n",
      "         [0.1608, 0.1647, 0.1725,  ..., 0.7412, 0.7333, 0.7294],\n",
      "         [0.1922, 0.1961, 0.1765,  ..., 0.7412, 0.7373, 0.7333]],\n",
      "\n",
      "        [[0.1294, 0.1373, 0.1333,  ..., 0.3255, 0.3176, 0.3255],\n",
      "         [0.1255, 0.1255, 0.1255,  ..., 0.3216, 0.3137, 0.3176],\n",
      "         [0.1255, 0.1176, 0.1216,  ..., 0.3176, 0.3137, 0.3098],\n",
      "         ...,\n",
      "         [0.1137, 0.1020, 0.1059,  ..., 0.4000, 0.3922, 0.3765],\n",
      "         [0.1137, 0.1176, 0.1255,  ..., 0.4039, 0.4000, 0.4000],\n",
      "         [0.1333, 0.1412, 0.1294,  ..., 0.4039, 0.4039, 0.4000]]]), 'question': 'What is this item', 'answers': 'unanswerable', 'captions': ['a person holding a bag filled with food']}\n"
     ]
    }
   ],
   "source": [
    "for item in pytorch_train:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a819752-0815-4fd4-8a52-1c77b4d34667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a pair of scissors sitting on top of a bed', 'a television screen with a pizza in front of it', 'a computer monitor sitting on top of a wooden desk', 'a bottle of alcohol sitting on the side of a street', 'a bottle of water sitting on top of a table', 'a room with a window and a vase of flowers', 'a microwave oven sitting on top of a counter', 'a black and white photo of a computer screen', 'a brown teddy bear wearing a red bow tie', 'a blurry image of a sunset with a cloudy sky', 'a black dog laying on the ground', 'a person sitting in front of a box filled with food', 'a glass with a liquid inside of it', 'a person holding a purple frisbee in their hand', 'a bottle of beer sitting on top of a counter', 'a sign that says \"don\\'t drink\" on it')]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['captions'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56d41fe9-57f1-49ce-8edf-9ce407f7a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, num_epochs, device, checkpoint_path=\"model_checkpointvizwiz.pth\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains the MultimodalVQAModel.\n",
    "\n",
    "    Arguments:\n",
    "    - model (nn.Module): The multimodal VQA model to train.\n",
    "    - train_loader (DataLoader): DataLoader providing batches of training data.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "    - num_epochs (int): Number of training epochs.\n",
    "    - device (torch.device): Device (CPU or GPU) for computation.\n",
    "    - checkpoint_path (str): Path to save model checkpoints.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            # Preprocess images, questions, and captions\n",
    "            images = batch[\"image\"].to(device)\n",
    "            \n",
    "            questions = question_caption_tokenizer(batch[\"question\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "            captions = [caption for group in batch[\"captions\"] for caption in group]\n",
    "            captions = question_caption_tokenizer(captions, padding=True, return_tensors=\"pt\").to(device)\n",
    "            answers = batch[\"answers\"]  # Ensure this is in the right format as well\n",
    "\n",
    "            # Tokenize answers\n",
    "            answer_encodings = answer_tokenizer(answers, padding=True, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(\n",
    "                image=images,\n",
    "                questions = questions,\n",
    "                captions = captions,\n",
    "                answers= answers\n",
    "            )\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Average loss per epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        \n",
    "        torch.save(model.state_dict(),checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "184daa8a-fde5-47d1-8428-b80930bffd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:46<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 8.9436\n",
      "Checkpoint saved at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:46<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 8.9780\n",
      "Checkpoint saved at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:46<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 9.0086\n",
      "Checkpoint saved at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:45<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 8.9891\n",
      "Checkpoint saved at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:45<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 9.0353\n",
      "Checkpoint saved at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:46<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 8.9583\n",
      "Checkpoint saved at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:46<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 8.9718\n",
      "Checkpoint saved at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:44<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 8.9747\n",
      "Checkpoint saved at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████| 924/924 [06:46<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 8.9784\n",
      "Checkpoint saved at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████| 924/924 [06:47<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 8.9512\n",
      "Checkpoint saved at epoch 10\n"
     ]
    }
   ],
   "source": [
    "train(modelvqa,train_loader,optimizer, 10, device, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4af51406-b680-464f-99e3-5f95a7a1e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validates the MultimodalVQAModel on the validation dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The trained multimodal VQA model to validate.\n",
    "    - val_loader (DataLoader): DataLoader providing batches of validation data.\n",
    "    - device (torch.device): Device (CPU or GPU) for computation.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Validation accuracy as a fraction between 0 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            # Preprocess images, questions, captions, and answers\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = question_caption_tokenizer(batch[\"question\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "            captions = [caption for group in batch[\"captions\"] for caption in group]\n",
    "            captions = question_caption_tokenizer(captions, padding=True, return_tensors=\"pt\").to(device)\n",
    "            answers = batch[\"answers\"]\n",
    "\n",
    "            # Tokenize answers\n",
    "            answer_encodings = answer_tokenizer(answers, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # Forward pass to get logits\n",
    "            _, logits = model(image=images, questions=questions, captions=captions, answers=answers)\n",
    "\n",
    "            # Get predicted answers (highest logit)\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            predicted_answers = answer_tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            print(predicted_answers)\n",
    "\n",
    "            # Calculate accuracy by comparing predicted answers with ground-truth answers\n",
    "            correct_predictions += sum([1 for pred, true in zip(predicted_answers, answers) if pred.strip().lower() == true.strip().lower()])\n",
    "            total_predictions += len(answers)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42a33824-e39e-49eb-962f-7c90b63e96e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   1%|▋                                                                      | 1/103 [00:01<02:57,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',.,, and of and', ',,,,,,,', ',,,,,,,', ',,,,,,\\n', ',,,\\n\\n\\n\\n', ',,,,,,\\n', ',,,\\n\\n\\n\\n', ',...,,,', ',,,,\\n\\n\\n', ',,, and and and and', ',,. and and and and', ',..,,,,', ',,,,,,,', ',,,,\\n\\n\\n', ',,,,,\\n\\n', ',,,,\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   2%|█▍                                                                     | 2/103 [00:03<03:19,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n,,,', '\\n,,\\n', ',,,,', ',,,,', ',,,\\n', \",'s,\\n\", ',,,,', ',,,,', ',,,,', ',,,,', ',,,,', ',,,,', ',,..', ',,,,', ',,,,', ',,,,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   3%|██                                                                     | 3/103 [00:05<03:07,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',....', ',., and,', ',,,,,', ',,,\\n\\n', ',,,,,', ',,,\\n\\n', ',,,\\n\\n', ',,..,', ',,,,,', ',,,,\\n', ',,,,\\n', ',,,,,', ',,, and and', ',,,..', ', of of..', ',,,\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   4%|██▊                                                                    | 4/103 [00:08<03:28,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n,,', ', of,', ',,,', ',,,', ',,.', ',,,', ',,,', ',..', ',,,', ',,,', ',,,', ',..', ',,.', ',,,', ',,,', ',,.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   5%|███▍                                                                   | 5/103 [00:10<03:41,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,\\n\\n', ',,,,,,', ',,,,,,', ',,,\\n\\n\\n', ',,,,,,', ',,,,\\n\\n', ',\\n,\\n\\n\\n', ',,,,,,', ',,, of of\\n', ',,,.,\\n', ',,,,,\\n', ',,,,,\\n', ',,,,. and', ',,,. and and', ',,,,,,', ', of.\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   6%|████▏                                                                  | 6/103 [00:13<03:49,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,\\n\\n\\n', ',,,,,,.', ',,,,,,\\n', ',,\\n\\n\\n\\n\\n', ',,,,,,\\n', ',,\\n\\n\\n\\n\\n', ',,,,\\n\\n\\n', ',,,,\\n\\n\\n', ',,,,,- and', ',,,,,\\n\\n', ',.,,,,\\n', ',,..\\n\\n\\n', ',....,\\n', ',,,,,,\\n', ',,\\n\\n\\n\\n\\n', '\\n,,\\n\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   7%|████▊                                                                  | 7/103 [00:14<03:21,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,..,,,\\n', '\\n of,,\\n,,,', '\\n of\\n\\n\\n\\n\\n\\n', ',,,\\n and and and and', ',,\\n\\n\\n\\n\\n\\n', ',,,,,\\n\\n\\n', ', of,,,,\\n\\n', ',..... of of', ',,,,,,\\n\\n', ',, of. and and and and', ',,,,\\n\\n\\n\\n', ',,,,. and and and', ',....,, and', ',,,,,,, music', ',,,\\n\\n\\n\\n\\n', ',,,,,\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   8%|█████▌                                                                 | 8/103 [00:15<02:47,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,,\\n\\n', ',,,,, of of', ',,,,,\\n\\n', ',,,,,,\\n', ',,,, of of of', ',,,\\n\\n\\n\\n', ',,,,,,\\n', ',....,\\n', ',,,,,\\n\\n', ',,,, game\\n\\n', ',,,,,,\\n', ',,,,,\\n\\n', ',,,\\n\\n\\n\\n', ',,,,,-\\n', ',.,,,,,', ',,,,,,,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   9%|██████▏                                                                | 9/103 [00:18<03:04,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,\\n\\n\\n\\n', ',,,,\\n\\n\\n\\n', ',,,,\\n\\n\\n\\n', ',,,,\\n\\n\\n\\n', ',,,,,,,,', ',,,,\\n\\n\\n\\n', ',,,,,\\n\\n\\n', ',...,,,\\n', ',,,,\\n\\n\\n\\n', ',,..\\n\\n\\n\\n', ',,,,,\\n\\n\\n', ',,,, and and and and', ',,..\\n\\n\\n\\n', ',,,,,\\n\\n\\n', ',,,,. and and and', ',,,,,,,,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  10%|██████▊                                                               | 10/103 [00:19<02:50,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,, of', ',,, of and and', ',,,,, of', ',,,,,,', ',.,,,,', ',,,,. and', ', of,,,,', ',,..,,', ',,,,,,', ',,,,\\n\\n', ',,....', ',,..\\n\\n', '\\n,,,\\n\\n', ',,,\\n\\n\\n', ',,,,,,', ',,, of and and']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|███████▍                                                              | 11/103 [00:21<02:52,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,,,, of of and and and', ',,,,\\n of of of of of of of', ',,,,,-,,, and and and', ',,,,,,\\n\\n\\n\\n\\n\\n', ',,,\\n\\n\\n\\n\\n\\n\\n\\n\\n', ',,,...\\n\\n\\n\\n\\n\\n', ',..,,,,,,,..', ',, of...\\n\\n\\n\\n\\n\\n', '\\n,\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', ',,,,,,,,,...', ',,......\\n\\n\\n\\n', ', of of....,\\n\\n\\n\\n', ',,,,,,,\\n\\n\\n\\n\\n', ',,,,.\\n\\n\\n\\n\\n\\n\\n', ',,,,, and and and....', ',,,,\\n\\n\\n\\n\\n\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|████████▏                                                             | 12/103 [00:23<02:42,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,,\\n\\n', ',...\\n\\n\\n', ',,,,,, and', ',,,,,, and', ',,\\n\\n\\n\\n\\n', ',,,,,,\\n', ',,,,\\n\\n\\n', ',,,,,,,', ',,,\\n\\n\\n\\n', ',,.\\n\\n\\n\\n', ',,,,,,,', ',,.. and and and', ',,,.\\n\\n\\n', ',,,,,,\\n', ',......', ',,,,,\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  13%|████████▊                                                             | 13/103 [00:26<03:12,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,,\\n\\n\\n\\n\\n\\n', ',.,,,,\\n\\n\\n\\n\\n', ',...\\n\\n\\n\\n\\n\\n\\n', ',....\\n\\n\\n\\n\\n\\n', ',,,,.,\\n\\n\\n\\n\\n', ',,,\\n\\n\\n\\n\\n,,,', ',.... and and and and and and', ',,, and and and and and and and and', ',,,,,,,,\\n\\n\\n', ',,,, of of of, and and and', ',,...,.\\n\\n\\n\\n', ',,,........', ',,,,\\n\\n\\n\\n\\n\\n\\n', ',...,\\n\\n\\n\\n\\n\\n', ',,,,,\\n\\n and and and and', ',,,,,\\n\\n\\n\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|█████████▌                                                            | 14/103 [00:28<03:17,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',,,,\\n\\n', ',,,,..', ',,,\\n\\n\\n', ',,,,,\\n', ',,,,,,', ',,,,,\\n', ',.,,,,', ',,..,\\n', ',,.\\n\\n\\n', '\\n of..\\n\\n', ',,,,\\n\\n', ',,,,\\n\\n', ',,,,,,', ',,,.,\\n', ',,,..\\n', ',,,,,,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  15%|██████████▏                                                           | 15/103 [00:30<03:11,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',.....\\n\\n', ',,,, music and and and', ',,.... and and', ',,,,,,,,', ',,,,,\\n\\n\\n', '\\n,,\\n\\n\\n\\n\\n', ',,,,,\\n\\n\\n', ',,,.,,,,', ',......\\n', ',,,,,\\n\\n\\n', ',,..,,\\n\\n', ',,,\\n\\n\\n\\n\\n', ', of.,,\\n\\n\\n', ',..,\\n\\n\\n\\n', ',,,,,,,\\n', ',,,,\\n\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  15%|██████████▏                                                           | 15/103 [00:31<03:05,  2.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(pytorch_valid, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelvqa\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mValidating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Preprocess images, questions, captions, and answers\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquestion_caption_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib64/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2746\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2746\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2747\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2742\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2727\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2725\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2726\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2727\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/formatting.py:407\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:112\u001b[0m, in \u001b[0;36mTorchFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    110\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    111\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 112\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    114\u001b[0m     batch[column_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate(batch[column_name])\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:95\u001b[0m, in \u001b[0;36mTorchFormatter.recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/utils/py_utils.py:512\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 512\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/utils/py_utils.py:373\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function([data_struct])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36mTorchFormatter._recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:95\u001b[0m, in \u001b[0;36mTorchFormatter.recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/utils/py_utils.py:484\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    483\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[0;32m--> 484\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    486\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36mTorchFormatter._recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:95\u001b[0m, in \u001b[0;36mTorchFormatter.recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/utils/py_utils.py:484\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    483\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[0;32m--> 484\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    486\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36mTorchFormatter._recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:95\u001b[0m, in \u001b[0;36mTorchFormatter.recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/utils/py_utils.py:484\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    483\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[0;32m--> 484\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    486\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:92\u001b[0m, in \u001b[0;36mTorchFormatter._recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:78\u001b[0m, in \u001b[0;36mTorchFormatter._tensorize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     75\u001b[0m             value \u001b[38;5;241m=\u001b[39m value[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m     77\u001b[0m         value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdefault_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_tensor_kwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_loader = DataLoader(pytorch_valid, batch_size=16, shuffle=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"CPU\")\n",
    "validate(modelvqa,valid_loader,device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
